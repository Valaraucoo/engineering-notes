# S3

- Amazon S3 allows storing objects (files) in buckets (directories)
- **Buckets must have globally unique name (across all regions and all accounts)**
- Buckets are defined in **region level** → it’s not global service
    - Each bucket is assigned to specific region
- Naming convention:
    - no-uppercase, no-underscore
    - 3-63 chars long
    - not an IP
    - must start with lowercase or number
    - must not start with prefix `xn—`
    - must not end with postfix `-s3alias`

S3 Objects:

- Each S3 object contains: key, values, metadata, tags, version ID
- **Objects (files) has a Key**
    - The name that you assign to an object.
    - You use the object key to retrieve the object.
    - The Key is the FULL Path to file: `s3://my-bucket/my-file.txt` etc.
    - The Key is composed of prefix + object name
        - `s3://my-bucket/folder/another-folder/file.txt`
        - `file.txt` is object name
        - `my-bucket/folder/another-folder/` is prefix
    - **There is no concept of directories**
- **Value** are the content of the body
    - Within a bucket, a key and version ID uniquely identify an object.
    - Max object size is 5TB
    - You must upload >5GB files using “multi-part upload” → then you upload n x 5GB parts
- **Object Metadata** — list of text key / value pairs → system of user metadata
- **Tags** — unicode key / value pair (up to 10)
- **Version ID** — if versioning is enabled
    - Within a bucket, a key and version ID uniquely identify an object.

S3 Security:

- **User-Based**
    - **IAM Policies** — which API calls should be allowed for a specific user from IAM
- **Resource-Based**
    - **Bucket Policies** — bucket wide rules from the s3 console — allows **cross account** (most common)
        - If you apply a bucket policy at the bucket level, you can define who can access (Principal element), which objects they can access (Resource element), and how they can access (Action element).
    - **Object Access Control List (ACL)** — fine grain
        - Use object ACLs to manage permissions only for specific scenarios and only if ACLs meet your needs better than IAM and S3 bucket policies.
        - Amazon S3 ACLs allow users to define only the following permissions sets: READ, WRITE, READ_ACP, WRITE_ACP, and FULL_CONTROL. Y
    - **Bucket Access Control List (ACL)** — less common
- IAM roles and resource-based policies delegate access across accounts only within a single partition.
    - For example, assume that you have an account in US West (N. California) in the standard `aws` partition.
    - You also have an account in China (Beijing) in the `aws-cn` partition.
    - You can't use an Amazon S3 resource-based policy in your account in China (Beijing) to allow access for users in your standard AWS account
    
- IAM entity can access s3 bucket/object if:
    - The user IAM permissions ALLOW it OR the resource policy ALLOW it
    - AND there’s no explicit DENY
- **Encryption**: encrypt objects in S3 using encryption keys
    
    

S3 Bucket Policies:

- JSON based policy
    - `Resources` → bucket and/or objects
    - `Effect` → Allow or Deny
    - `Actions` → set of APIs to allow or deny by policy
    - `Principal` → The account or user to apply the policy to (`*` is everything)
- You can use S3 Bucket Policy to:
    - Grant public access to the bucket
    - Force objects to be encrypted at upload
    - Grant access to another account (cross-account)
    - etc.

<aside>
ℹ️ Explicit DENY in an IAM Policy will take precedence over an S3 bucket policy.

</aside>

Amazon S3 — Static Website Hosting:

- S3 can host static websites and have them accessible on the Internet
- The website URL will be:
    - `[http://bucket-name.s3-website-aws-region.amazonaws.com](http://bucket-name.s3-website-aws-region.amazonaws.com)` or
    - `[http://bucket-name.s3-website.aws-region.amazonaws.com](http://bucket-name.s3-website-aws-region.amazonaws.com)`

S3 Versioning:

- You can version your files in S3
- It is enabled in **bucket level**
- The same keys will be overridden and the version will change “1, 2, 3, …”
- It is best practice to version your buckets
    - Protect against unintended deletion
    - There is an ability to restore specified version
    - Easy roll back to previous version
- If versioning is disabled → version: none

S3 Replication: CRR, SRR

- **CRR → Cross Region Replication**
- **SRR → Same Region Replication**
- **Asynchronous replication** between buckets (in another region/AZ)
    - Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets
    - You can replicate objects to a **single** destination bucket or to **multiple** destination buckets
    - The destination buckets can be in **different AWS Regions** or **within the same Region** as the source bucket→ **Buckets can be in two different accounts!**
- **Must enable versioning** in source and destination buckets.
- **Must give proper IAM permissions to S3**
    - To read/write from specified buckets
- After enabling Replication → **only new objects are replicated**
    - To automatically replicate **new objects** as they are written to the bucket, use live replication, such as **CRR or SRR.**
    - Optionally, you can replicate existing objects using **S3 Batch Replication**
- **Can replicate delete markers** from source to target buckets
    - If versioning is enabled → after deletion in bucket, you can see deletion marker
    - We can replicate those markers to another bucket too
- **There is no chaining in replication**
- **Version ID are replicated between buckets**

S3 Storage Classes:

- Amazon S3 Standard - General Purpose
- Amazon S3 Standard - Infrequent Access (IA)
- Amazon S3 One Zone - Infrequent Access
- Amazon S3 Glacier Instant Retrieval
- Amazon S3 Glacier Flexible Retrieval
- Amazon S3 Glacier Deep Archive
- Amazon S3 Intelligent Tiering

- We can move objects between classes manually or using lifecycle configuration

S3 Durability and Availability:

- Durability — how many times an object is going to be lost by AMW S3
    - AWS S3 has very high durability → 11 * 9’s of objects across multiple AZs
    - **Same for all storage classes**
- Availability — measures how readily available a service is
    - **Varies depending on storage classes**
    - Example: S3 Standard has 99.99% availability → not available 53mins per year

| Storage Class | Availability / Durability | Use Cases | Latency / Throughput | Extra | Costs |
| --- | --- | --- | --- | --- | --- |
| Standard - General Purpose | 99.99% / 11 9’s | Used for frequently accessed data,
Big Data, Apps, Content Distribution | Low / High
 | Sustain 2 concurrent facility failures | - |
| Standard - Infrequent Access | 99.9% / 11 9’s | Used for less frequently accessed data, but requires rapid access when needed
Disaster Recovery and Backups |  |  | Lower than S3 Standard |
| One Zone - Infrequent Access | 99.5% 

Durability: 11 9’s in single AZ | Used for less frequently accessed data, but requires rapid access when needed
Storing secondary Backups, copies to re-create |  |  | Lower than S3 Standard |
| S3 Glacier Instant Retrieval | 99.9% / 11 9’s | Milliseconds Retrieval (instant)
Minimum Storage Duration: 90 days
Fast backup |  |  | Low-cost, Pricing for Storage + retrieval cost |
| S3 Glacier Flexible Retrieval | 99.99% / 11 9’s | Has 3 flexibility tiers for retrievals:
- Expedited (1-5 min)
- Standard (3-5 hours)
- Bulk (5-12 hours) FREE
Minimum Storage Duration: 90 days |  |  | Low-cost, Pricing for Storage + retrieval cost |
| S3 Glacier Deep Archive | 99.99% / 11 9’s | For long-term storage
Has 2 tiers for retrievals:
- Standard (12 hours)
- Bulk (48 hours)
Minimum Storage Duration: 180 days |  |  | Low-cost, Pricing for Storage + retrieval cost |
| S3 Intelligent Tiering | 99.9% / 11 9’s | Automatically moves objects between access tiers based on usage:
- Frequent Access Tier (default)
- Infrequent Access Tier - objects not accessed for 30 days
- Archive Instant Access Tier - objects not accessed for 90 days
- Archive Access Tier (optional) - configurable from 90 to 700+ days
- Deep Archive Access Tier (optional) - configurable from 180 to 700+ days |  |  | Small monthly monitoring and auto-tiering fee |

![Untitled](S3%2043a656df81394e5c8dc7e35a380d6895/Untitled.png)

S3 Lifecycle Rules:

- To manage your objects so that they are stored cost effectively throughout their lifecycle, configure their *Amazon S3 Lifecycle*
- An *S3 Lifecycle configuration* is a set of rules that define actions that Amazon S3 applies to a group of objects.
- **With S3 Lifecycle configuration rules, you can tell Amazon S3 to transition objects to less-expensive storage classes, or archive or delete them.**
- Two types of actions:
    - **Transition actions** – These actions define when objects transition to another storage class.
        - e.g. from S3 Standard to Glacier after 180 days
    - **Expiration actions** – configure objects to expire (delete) after some time.
        - e.g. **delete old versions of files** (if versioning is enabled)
        - e.g. delete incomplete multipart uploads
- Rules can be created for a certain **prefix**, e.g. `s3://mybucket/my-files/*`
- Rules can be created for certain **tags**, e.g. `Department=Finance`
- Example usage:
    - **Logs storing**: If you upload periodic logs to a bucket, your application might need them for a week or a month. After that, you might want to delete them.

S3 Analytics — Storage Classes Analysis: 

- Help you decide when to transition objects to the right storage class
- **Recommendations for Standard and Standard IA**
    - Does not work for One-Zone IA or Glacier
- S3 bucket has S3 Analytics runs on top of it → S3 Analytics generates CSV report with statistics and recommendations
- Report is updated Daily → 24-48h to start seeing analytics

S3 Event Notifications:

- Events are all operation on AWS API, e.g. `S3:ObjectCreated`, `S3:ObjectRemoved`
- Object name filtering is possible `*.jpg`
- Use case: generate thumbnails  of images uploaded to S3
- **Can create as many “S3 Events” as desired** (and send them wherever you want)
- S3 event notifications typically deliver events in seconds, but can sometimes take a minute or longer
- **If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent**
    - If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent.
    - If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket.
    - With versioning, every successful write will create a new version of your object and will also send event notification.
    - **If you want to ensure that an event notification is sent for every successful write, you should enable versioning on your bucket.**
- **Main destinations for events: SNS topic, SQS queue, Lambda, Event Bridge**
- Amazon Event Bridge:
    - All S3 events end up in Event Bridge
    - From Event Bridge, you can set up rules to send event to over 18 AWS service destinations
    - Advanced filtering + JSON rules (metadata, object size, name, etc.)
    - Advanced capability — archive, replay events, reliable delivery

S3 Performance:

- By default, S3 scales to high requests rate and very low latency (100-200ms)
- **Your application can achieve at least 3.500 PUT/POST/DELETE and 5.500 GET/HEAD requests per second per prefix in a bucket**
- There is no limit to the number of prefixes in a bucket
- Example:
    - Object path: `bucket/folder/sub-folder/file`
        - prefix → `folder/sub/` (between bucket name and object name)
    - For each different prefix you can have different limits

How to optimize S3 Performance?

- **Multipart upload:**
    - **Recommended for files > 100MB, Must use for files  > 5GB**
    - **Parallel** uploads files (speed up transfer)
    - Multipart upload → divide file into parts → upload each part in parallel
    - S3 put all file parts together into one big file
- **S3 Transfer Acceleration:**
    - **Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target location/region**
        - There is more edge locations than regions - user upload file to edge location and then file will be transfered via private network to target location
    - Compatible with Multipart upload
    - S3 Transfer Acceleration (USA) → upload to USA edge location → AWS in private fast network will transfer file to Australia S3 bucket
- **S3 Byte-Range Fetches:**
    - Parallelize GETs by requesting specific byte range
    - Better resilience in case of failures → you must retry smaller byte range
    - **Can be used to speed up downloads**
    - **Can be used to retrieve only partial data (for example head of the files)**
    

S3 Select & Glacier Select:

- Retrieve less data using SQL by performing **server-side filtering**
- Can filter by rows & columns (simple SQL statements)
- Less network transfer, less CPU cost client-side
- S3 Select enables applications to retrieve **only a subset of data** from an object by using simple SQL expressions
- By using S3 Select to retrieve only the **data needed by your application**, you can achieve drastic performance increases.

![Untitled](S3%2043a656df81394e5c8dc7e35a380d6895/Untitled%201.png)

S3 User-Defined Object Metadata & S3 Object Tags:

- S3 User-Defined Object Metadata → when uploading an object, you can also assign metadata
    - Name-value pairs
    - User-defined metadata names must begin with `x-amz-meta-`
    - metadata can be retrieved while retrieving the object
    - e.g. `x-amz-meta-origin=paris`
- S3 Object Tags:
    - Key-value pairs for objects in S3
    - useful for fine-grained permissions (only access specific objects with specific tags)
    - useful for analytics purposes (using s3 analytics to group by tags)
    - e.g. `Project=Blue`
- ********************************************************************************************************You cannot search the object metadata or object tags********************************************************************************************************
- Instead, you must use an external DB as a search index such as DynamoDB

### S3 Security

S3 Object Encryption:

- You can encrypt objects in S3 buckets using one of 4 methods:
- **Server-Side Encryption** (SSE):
    - S3 encrypts your data at the object level as it writes it to disks in AWS data centers and decrypts it for you when you access it
    - Server-Side Encryption with S3-Managed Keys (**SSE-S3**)
        - Encrypt S3 objects using keys handled/managed/owned by AWS
    - Server-Side Encryption with KMS Keys Stored in AWS KMS (**SSE-KMS**)
        - Using AWS KMS to manage encryption keys
    - Server-Side Encryption with Customer Provided Keys (**SSE-C**)
- **Client Side Encryption**

- **Server-Side Encryption with S3-Managed Keys (SSE-S3)**
    - In this method, the keys to encrypt and decrypt data are generated and managed by Amazon S3.
        - The encryption is using a key that’s handled and managed by AWS
    - Objects are encrypted server-side by AWS
    - Encryption type: **AES-256**
    - You must set Header `"x-amz-server-side-encryption": "AES256"` to request S3 to encrypt the object using SSE-S3
    - When you upload object to S3, AWS will pair it with the S3 owned key and encrypt object
        - Each object is encrypted with a unique key, and that key is then encrypted with a master key that is regularly rotated by Amazon S3.
- **Server-Side Encryption with KMS Keys Stored in AWS KMS (SSE-KMS)**
    - The encryption is using keys handled and managed by AWS KMS (Key Management Service)
        - KMS advantages: user control + audit keys (you can monitor keys usage)
        - For example, you can view separate keys, edit control policies, and follow the keys in AWS CloudTrail
    - Objects are encrypted server-side by AWS
    - You must set Header `"x-amz-server-side-encryption": "aws:kms"` to request S3 to encrypt the object using SSE-S3
    - **To read object from S3 you need access to object and access to underlying KMS Key that was used to encrypt this object**
    - Limitations:
        - When you upload object → you call GenerateDataKey KMS API
        - When you download object → you call Decrypt KMS API
        - You can be throttled!
- **Server-Side Encryption with Customer Provided Keys (SSE-C)**
    - The encryption is using fully-managed keys by the Customer outside of AWS
    - **S3 does not store the encryption keys you provided**
        - AWS does not store these keys, which means that the customer must securely store them and provide them with each request.
    - **HTTPS must be used**
    - Encryption Key must be provided in **HTTP headers**, for every HTTP request made
        - User must pass encryption key in header while uploading file
        - while reading files user must also specify key in header
    
    ![Untitled](S3%2043a656df81394e5c8dc7e35a380d6895/Untitled%202.png)
    

<aside>
💡 Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it writes to disks, and decryption, when you access your objects.

</aside>

- **Client Side Encryption**
    - Use Client libraries such as **Amazon S3 Client-Side encryption Library**
    - Clients must encrypt data themselves before sending to S3
    - Clients must decrypt data themselves when retrieving from S3
    - Client fully-manages the keys and encryption cycle
    
- Encryption in Transit SSL/TLS
    - Encryption in flight → SSL/TLS
    - Amazon S3 exposes two endpoints:
        - **HTTP Endpoint** — non encrypted
        - **HTTPS Endpoint** — encrypted in flight
    - **HTTPS is mandatory for SSE-C**

Default Encryption vs. Bucket Policy

- One way to “force encryption” is to use a **bucket policy** and refuse any API call to PUT an S3 Object without encryption headers
- **Another option is to use Default Encryption in S3**
    - Even object is uploaded without encrypted manner it will be forced to be encrypted
- You can set the **default encryption** behavior on an Amazon S3 bucket so that all objects are encrypted when they are stored in the bucket.
    - Without default encryption, to encrypt all objects stored in a bucket, you must include encryption information (header) with every object storage request.
    - You must also set up an Amazon S3 **bucket policy** to **reject storage requests** that don't include encryption information.
- **Note: Bucket Policies are always evaluated before default encryption**

S3 Cors:

- If a client makes a cross-origin request on our S3 bucket, we need to enable the correct CORS headers
- You can allow for a specific origin or for * (all origins)
- In S3 which contains assets other origin want to download you must specify Permissions > Cors Settings

S3 MFA Delete:

- MFA will be required to:
    - Permanently delete an object version
    - Suspend Versioning on the buckets
- MFA won’t be required to:
    - Enable Versioning
    - List deleted Versions
- **To use MFA Delete → Versioning must be enabled on the bucket**
- **Only bucket owner (root account) can enable/disable MFA Delete**

S3 Access Logs:

- For audit purposes, you may want to log all access to s3 buckets
    - Any request authorized/or not will be logged into another s3 bucket
- **The target logging bucket must be in the same AWS region**
- S3 Access Logs: Warning
    - **Do not set your logging bucket to be the monitored bucket — It will create a logging loop**

S3 Pre-Signed URLs:

- The object owner can optionally share (private) objects with others by creating a **presigned URL**, using their own security credentials, to grant time-limited permission to download the objects.
- When you create a presigned URL for your object:
    - You must provide your **security credentials** and then specify a **bucket name**, an object **key**, an **HTTP** **method** (GET to download the object), and an **expiration date and time**.
- Anyone who receives the presigned URL can then access the object.
- You can generate pre-signed URL using: S3 Console, AWS CLI or SDK
- URL has expiration time:
    - **S3 Console** → 1min - 720 min (12 hours)
    - **AWS CLI / SDK** → (`expiresIn` parameter) default is 3600 seconds (1 hour) up to 7 days
- Use case: temporary access to specified file

S3 Access Points:

- Amazon S3 access points simplify data access for any AWS service or customer application that stores data in S3.
- Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations.
- **Each Access Point gets its own DNS and policy to limit who can access it**
    - A specific IAM user / group
    - One policy per Access Point → **Easier to manage than complex bucket policies**
- *Another option is to create complex bucket policies to limit access*

S3 Object Lambda Access Point:

- You can use AWS Lambda Function to modify/change the object before it is retrieved by the caller application
- S3 Object Lambda uses AWS Lambda functions to automatically process the output of a standard S3 GET, HEAD, and LIST request.
- Only one S3 bucket is needed on top of which we create S3 Access Point and S3 Object Lambda Access Points